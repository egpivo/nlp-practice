{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "from nlp_practice.case.translation.data.dataloader import PairDataLoader\n",
    "from nlp_practice.model.decoder import AttentionDecoderRNN, DecoderRNN\n",
    "from nlp_practice.model.encoder import EncoderRNN\n",
    "from nlp_practice.case.translation.training.trainer import Seq2SeqTrainer\n",
    "from nlp_practice.case.translation.inference.predictor import Predictor\n",
    "from nlp_practice.case.translation.data.preprocessor import Preprocessor\n",
    "from nlp_practice.case.translation.evalution.evaluator import Evaluator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "LOGGER = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "hidden_size = 128\n",
    "dropout_rate = 0.1\n",
    "num_epochs = 500\n",
    "training_rate = 0.8\n",
    "learning_rate = 0.001\n",
    "device = \"cpu\"\n",
    "data_base_path = \"../../data\"\n",
    "first_language = \"eng\"\n",
    "second_language = \"fra\" \n",
    "normal_checkpoint = \"normal_seq2seq.pt\"\n",
    "attention_checkpoint = \"attention_seq2seq.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_language, output_language, pairs = Preprocessor(\n",
    "    base_path=data_base_path,\n",
    "    first_language=first_language,\n",
    "    second_language=second_language,\n",
    "    does_reverse=True,\n",
    ").process()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderRNN(\n",
    "    input_size=input_language.num_words,\n",
    "    hidden_size=hidden_size,\n",
    "    dropout_rate=dropout_rate,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "normal_decoder = DecoderRNN(\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_language.num_words,\n",
    "    dropout_rate=dropout_rate,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "attention_decoder = AttentionDecoderRNN(\n",
    "    hidden_size=hidden_size,\n",
    "    output_size=output_language.num_words,\n",
    "    dropout_rate=dropout_rate,\n",
    "    device=device,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = PairDataLoader(\n",
    "    pairs=pairs,\n",
    "    input_language=input_language,\n",
    "    output_language=output_language,\n",
    "    training_rate=training_rate,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    ").train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_trainer = Seq2SeqTrainer(\n",
    "    train_dataloader=train_dataloader,\n",
    "    encoder=encoder,\n",
    "    decoder=normal_decoder,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "normal_loss = normal_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_trainer = Seq2SeqTrainer(\n",
    "    train_dataloader=train_dataloader,\n",
    "    encoder=encoder,\n",
    "    decoder=attention_decoder,\n",
    "    num_epochs=num_epochs,\n",
    "    learning_rate=learning_rate,\n",
    ")\n",
    "attention_loss = attention_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "This section presents a comprehensive evaluation of the training results, examining both inference outputs and relevant metrics, including their associated losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Loss Comparison\n",
    "Explore the cross-entropy losses of both models across different epochs for a thorough understanding of their training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "sns.lineplot(normal_loss, ax=axes[0])\n",
    "axes[0].set_xlabel('Epochs')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Loss: Without-Attention Model')\n",
    "\n",
    "sns.lineplot(attention_loss, ax=axes[1])\n",
    "axes[1].set_xlabel('Epochs')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].set_title('Loss: With Attention Model')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Inference\n",
    "Utilizing the trained encoder and decoder models, we can proficiently translate input language sequences. To illustrate the translation quality, we showcase the capabilities of these models by randomly selecting three sentences for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_predictor = Predictor(encoder, normal_decoder, input_language, output_language)\n",
    "attention_predictor = Predictor(encoder, attention_decoder, input_language, output_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence, answer = random.choice(pairs)\n",
    "LOGGER.info(f\"Translate {input_sentence!r}\")\n",
    "LOGGER.info(f\"True: {answer!r}\")\n",
    "LOGGER.info(f\"Result without attention: {' '.join(normal_predictor.translate(input_sentence))!r}\")\n",
    "LOGGER.info(f\"Result with attention: {' '.join(attention_predictor.translate(input_sentence))!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence, answer = random.choice(pairs)\n",
    "LOGGER.info(f\"Translate {input_sentence!r}\")\n",
    "LOGGER.info(f\"True: {answer!r}\")\n",
    "LOGGER.info(f\"Result without attention: {' '.join(normal_predictor.translate(input_sentence))!r}\")\n",
    "LOGGER.info(f\"Result with attention: {' '.join(attention_predictor.translate(input_sentence))!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence, answer = random.choice(pairs)\n",
    "LOGGER.info(f\"Translate {input_sentence!r}\")\n",
    "LOGGER.info(f\"True: {answer!r}\")\n",
    "LOGGER.info(f\"Result without attention: {' '.join(normal_predictor.translate(input_sentence))!r}\")\n",
    "LOGGER.info(f\"Result with attention: {' '.join(attention_predictor.translate(input_sentence))!r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Metric Evaluation using the Test Dataset\n",
    "In assessing the performance of our models, we employ several key metrics, each offering unique insights into their capabilities.\n",
    "\n",
    "- **Accuracy:** This metric gauges the exact match between the model's predictions and the actual results, providing a straightforward measure of correctness.\n",
    "\n",
    "- **[ROUGE-1](https://en.wikipedia.org/wiki/ROUGE_(metric)):** Focused on unigrams, ROUGE-1 evaluates the alignment between the model's output and the reference text at the level of individual words. Precision, recall, and the F1 score within the ROUGE-1 framework offer distinct perspectives on the model's effectiveness in capturing relevant information from the reference, particularly at the granularity of unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = PairDataLoader(\n",
    "    pairs=pairs,\n",
    "    input_language=input_language,\n",
    "    output_language=output_language,\n",
    "    training_rate=training_rate,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    ").test_dataloader\n",
    "\n",
    "normal_evaluator = Evaluator(test_dataloader, normal_predictor)\n",
    "attention_evaluator = Evaluator(test_dataloader, attention_predictor)\n",
    "\n",
    "result_pdf = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"model\": [\"Without-Attention\", \"With-Attention\"],\n",
    "        \"accuracy\": [normal_evaluator.accuracy, attention_evaluator.accuracy],\n",
    "        \"rouge-1-precision\": [normal_evaluator.rouge1_precision, attention_evaluator.rouge1_precision],\n",
    "        \"rouge-1-recall\": [normal_evaluator.rouge1_recall, attention_evaluator.rouge1_recall],\n",
    "        \"rouge-1-f1\": [normal_evaluator.rouge1_f1, attention_evaluator.rouge1_f1],\n",
    "    }\n",
    ")\n",
    "result_melted = pd.melt(result_pdf, id_vars=\"model\", var_name=\"metric\", value_name=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\n",
    "\n",
    "metrics = result_pdf.columns[1:]\n",
    "for metric, ax in zip(metrics, axes.flatten()):\n",
    "    sns.barplot(x=\"model\", y=\"value\", data=result_melted[result_melted[\"metric\"] == metric], ax=ax)\n",
    "    ax.set(title=f\"{metric.capitalize()} Comparison\", xlabel=\"Model\", ylabel=\"Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
